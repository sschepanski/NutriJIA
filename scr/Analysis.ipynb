{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Table of Content](#table-of-content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0-General\n",
    "[Back to Table of Content](#table-of-content)\n",
    "# NutriJIA – Feasibility, Safety, and Exploratory Clinical Analysis of a Nutritional Intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[Back to Table of Content](#table-of-content)\n",
    "\n",
    "Nutritional interventions in pediatric and adolescent populations are increasingly explored as low-threshold strategies to support health and disease management. Before drawing conclusions about clinical effectiveness, it is essential to establish whether an intervention is **feasible**, **well-adhered to**, and **safe**—especially when biological safety markers and anthropometric parameters may change over time.\n",
    "\n",
    "NutriJIA evaluates a structured nutritional intervention compared to a control condition with repeated assessments at predefined time points, most prominently:\n",
    "\n",
    "- baseline visit: `visite_0_wo0`\n",
    "- post-intervention follow-up: `visite_1_wo12`\n",
    "\n",
    "In this notebook, feasibility is operationalised primarily through **adherence** metrics. Safety is evaluated using anthropometric and laboratory markers, including:\n",
    "\n",
    "- weight / BMI\n",
    "- Vitamin B12\n",
    "- transferrin saturation\n",
    "- zinc\n",
    "- beta-carotene\n",
    "\n",
    "The analyses focus on within-person change from baseline to post-intervention and on between-group differences (intervention vs. control), typically with baseline adjustment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "[Back to Table of Content](#table-of-content)\n",
    "\n",
    "The primary goal of this analysis is **not** to maximize predictive performance, but to provide a transparent, reproducible evaluation of **feasibility and safety** and to generate **exploratory clinical signals** that can inform future confirmatory studies.\n",
    "\n",
    "More specifically, we aim to:\n",
    "\n",
    "1. Construct an analysis-ready participant-level dataset with:\n",
    "   - group assignment (intervention vs. control)\n",
    "   - adherence metrics (definition depends on available data)\n",
    "   - safety endpoints (weight/BMI, Vitamin B12, transferrin saturation, zinc, beta-carotene)\n",
    "   - key baseline covariates for adjustment (as pre-specified)\n",
    "\n",
    "2. Perform the **main feasibility and safety analyses**:\n",
    "   - Quantify adherence and summarise feasibility descriptively (primary).\n",
    "   - Compare safety markers from baseline to post-intervention between groups using baseline-adjusted models (primary).\n",
    "\n",
    "3. Perform **exploratory clinical analyses** (secondary):\n",
    "   - Explore additional outcomes (if available) using the same baseline-adjusted framework.\n",
    "   - Treat all non-safety endpoints as hypothesis-generating.\n",
    "\n",
    "4. Integrate results into a coherent interpretation:\n",
    "   - Emphasise feasibility/safety conclusions and uncertainty.\n",
    "   - Clearly separate primary (feasibility/safety) from exploratory findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodological Role\n",
    "[Back to Table of Content](#table-of-content)\n",
    "\n",
    "This notebook is the central, reproducible record of:\n",
    "\n",
    "- how raw study exports are cleaned and reshaped,\n",
    "- how adherence, safety endpoints, and covariates are defined, and\n",
    "- how baseline-adjusted group comparisons are implemented and reported.\n",
    "\n",
    "Structurally, we distinguish:\n",
    "\n",
    "1. **Data processing**\n",
    "   - Import and harmonise visit-level data\n",
    "   - Derive an analysis-ready dataset (e.g., one row per participant per visit and/or one row per participant with change scores)\n",
    "\n",
    "2. **Exploratory data analysis**\n",
    "   - Missingness and distributions\n",
    "   - Study characteristics and baseline comparability (Table 1-style summary)\n",
    "\n",
    "3. **Primary analyses: feasibility & safety**\n",
    "   - Adherence summaries (feasibility)\n",
    "   - Baseline-adjusted between-group comparisons for safety endpoints\n",
    "\n",
    "4. **Secondary analyses: exploratory clinical outcomes**\n",
    "   - Additional baseline-adjusted models (clearly labeled exploratory)\n",
    "   - Sensitivity analyses (e.g., alternative adherence definitions, missing-data scenarios)\n",
    "\n",
    "5. **Interpretation**\n",
    "   - Summary of feasibility and safety evidence\n",
    "   - Transparent reporting of limitations and implications for future trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "[Back to Table of Content](#table-of-content)\n",
    "\n",
    "Statistical analysis and data preparation were conducted by **Dr. Steven Ngandeu Schepanski**, who also oversaw the development of this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup: Imports & Paths\n",
    "[Back to Table of Content](#table-of-content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define project paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The user specified the intended working directory on the local machine\n",
    "# On your laptop this should exist and will be used for reading and writing\n",
    "# In other environments the directory may not exist, so we fall back safely\n",
    "PROJECT_DIR = Path(\"/Users/stevenschepanski/Documents/04_ANALYSIS/NutriJIA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The notebook location is documented here so that anyone reading the code\n",
    "# understands where it is expected to live in the project structure\n",
    "NOTEBOOK_DIR = PROJECT_DIR / \"scr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data file path inside the project structure\n",
    "DATA_FILE = PROJECT_DIR / \"data\" / \"NutriJIA_SPSS_Showoff.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory used for this run\n",
      "/Users/stevenschepanski/Documents/04_ANALYSIS/NutriJIA\n"
     ]
    }
   ],
   "source": [
    "# If the project directory does not exist, fall back to the current folder\n",
    "# This makes the script robust when someone runs it in a different environment\n",
    "if PROJECT_DIR.exists():\n",
    "    WORK_DIR = PROJECT_DIR\n",
    "else:\n",
    "    WORK_DIR = Path.cwd()\n",
    "\n",
    "print(\"Working directory used for this run\")\n",
    "print(WORK_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas display preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These settings improve readability when inspecting intermediate tables\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw long data shape\n",
      "(115, 16)\n",
      "Raw long data columns\n",
      "['study_id', 'time', 'grp', 'kg', 'kg_percentile', 'kl', 'kl_percentile', 'bmi', 'bmi_percentile', 'globarztvas', 'globpatvas', 'tv_adhrz', 'Transferrin_Sättigung_percent', 'Zink_HP_microgramm_per_l', 'Vitamin_B12_nanogramm_per_l', 'beta_carotin_microgramm_per_l']\n"
     ]
    }
   ],
   "source": [
    "# Read the single sheet Excel file\n",
    "# If the file contains only one sheet, pandas will read it by default\n",
    "# We also explicitly convert column names to strings to avoid edge cases\n",
    "df_long = pd.read_excel(DATA_FILE)\n",
    "df_long.columns = [str(c) for c in df_long.columns]\n",
    "\n",
    "print(\"Raw long data shape\")\n",
    "print(df_long.shape)\n",
    "\n",
    "print(\"Raw long data columns\")\n",
    "print(df_long.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic validation of the long format structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates detected for study_id and time\n",
      "Number of unique people\n",
      "23\n",
      "Time point distribution\n",
      "time\n",
      "V0     23\n",
      "V1     23\n",
      "V2     23\n",
      "TV1    23\n",
      "TV2    23\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# The expected structure is one row per person per time point\n",
    "# We verify that the combination of study_id and time is unique\n",
    "# If it is not unique, pivoting would require aggregation which is risky\n",
    "key_cols = [\"study_id\", \"time\"]\n",
    "\n",
    "dup_mask = df_long.duplicated(subset=key_cols, keep=False)\n",
    "if dup_mask.any():\n",
    "    print(\"Warning duplicate rows detected for study_id and time\")\n",
    "    print(df_long.loc[dup_mask, key_cols + [\"grp\"]].sort_values(key_cols).head(20))\n",
    "else:\n",
    "    print(\"No duplicates detected for study_id and time\")\n",
    "\n",
    "print(\"Number of unique people\")\n",
    "print(df_long[\"study_id\"].nunique())\n",
    "\n",
    "print(\"Time point distribution\")\n",
    "print(df_long[\"time\"].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derive a clean group variable from grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functin to extract group assignment from the grp column\n",
    "# The grp column encodes group assignment using a pattern like\n",
    "# nJIA-P-0-01  for control\n",
    "# nJIA-P-1-02  for intervention\n",
    "#\n",
    "# There may be minor typos such as missing dashes\n",
    "# We will extract the first standalone 0 or 1 after the nJIA-P marker\n",
    "# If extraction fails, group will be missing and we will flag those rows\n",
    "\n",
    "def extract_group_from_grp(value):\n",
    "    \"\"\"\n",
    "    Extract group indicator from the grp string.\n",
    "\n",
    "    Expected patterns include\n",
    "    nJIA-P-0-01\n",
    "    nJIA-P-1-02\n",
    "\n",
    "    Also tolerates minor formatting issues such as missing a dash\n",
    "    nJIA-P1-15\n",
    "\n",
    "    Returns\n",
    "    0 for control\n",
    "    1 for intervention\n",
    "    np.nan if parsing fails\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "\n",
    "    s = str(value).strip()\n",
    "\n",
    "    # First attempt\n",
    "    # Look for nJIA then P then any separators then capture 0 or 1\n",
    "    m = re.search(r\"nJIA\\s*[-_]*\\s*P\\s*[-_]*\\s*([01])\", s, flags=re.IGNORECASE)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "\n",
    "    # Second attempt\n",
    "    # A more permissive fallback that captures any 0 or 1 that is surrounded by separators\n",
    "    m2 = re.search(r\"(?:^|[-_])([01])(?:[-_]|$)\", s)\n",
    "    if m2:\n",
    "        return int(m2.group(1))\n",
    "\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the extraction function to create a new group_code column\n",
    "df_long[\"group_code\"] = df_long[\"grp\"].apply(extract_group_from_grp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map numeric code to human readable labels\n",
    "# Using explicit labels makes later tables easier to interpret\n",
    "df_long[\"group\"] = df_long[\"group_code\"].map({0: \"control\", 1: \"intervention\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with missing group after parsing\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check whether any rows failed parsing\n",
    "n_missing_group = df_long[\"group\"].isna().sum()\n",
    "print(\"Rows with missing group after parsing\")\n",
    "print(n_missing_group)\n",
    "\n",
    "if n_missing_group > 0:\n",
    "    print(\"Examples of grp values that could not be parsed\")\n",
    "    print(df_long.loc[df_long[\"group\"].isna(), [\"study_id\", \"time\", \"grp\"]].drop_duplicates().head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group assignment appears consistent within study_id\n"
     ]
    }
   ],
   "source": [
    "# Ensure each study_id has a single consistent group assignment\n",
    "# If a study_id appears with both control and intervention, that is a data issue\n",
    "group_by_id = (\n",
    "    df_long[[\"study_id\", \"group\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby(\"study_id\")[\"group\"]\n",
    "    .nunique(dropna=False)\n",
    ")\n",
    "\n",
    "inconsistent_ids = group_by_id[group_by_id > 1].index.tolist()\n",
    "if len(inconsistent_ids) > 0:\n",
    "    print(\"Warning inconsistent group assignment detected for these study_id values\")\n",
    "    print(inconsistent_ids)\n",
    "    print(df_long.loc[df_long[\"study_id\"].isin(inconsistent_ids), [\"study_id\", \"grp\", \"group\"]].drop_duplicates())\n",
    "else:\n",
    "    print(\"Group assignment appears consistent within study_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform long to wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value columns to be pivoted into wide format\n",
      "['kg', 'kg_percentile', 'kl', 'kl_percentile', 'bmi', 'bmi_percentile', 'globarztvas', 'globpatvas', 'tv_adhrz', 'Transferrin_Sättigung_percent', 'Zink_HP_microgramm_per_l', 'Vitamin_B12_nanogramm_per_l', 'beta_carotin_microgramm_per_l']\n"
     ]
    }
   ],
   "source": [
    "# Identify value columns\n",
    "# We keep study_id, time, and group as identifiers and pivot everything else\n",
    "id_cols = [\"study_id\", \"group\", \"time\", \"grp\", \"group_code\"]\n",
    "value_cols = [c for c in df_long.columns if c not in id_cols]\n",
    "\n",
    "print(\"Value columns to be pivoted into wide format\")\n",
    "print(value_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot to wide format\n",
    "# This creates one row per study_id and group\n",
    "# Each measurement becomes a set of time specific columns\n",
    "# Example resulting column name\n",
    "# kg_V0\n",
    "# bmi_V1\n",
    "df_wide = df_long.pivot_table(\n",
    "    index=[\"study_id\", \"group\"],\n",
    "    columns=\"time\",\n",
    "    values=value_cols,\n",
    "    aggfunc=\"first\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide data shape\n",
      "(23, 40)\n",
      "Wide data preview\n",
      "   study_id         group Transferrin_Sättigung_percent_V0 Transferrin_Sättigung_percent_V1 Transferrin_Sättigung_percent_V2 Vitamin_B12_nanogramm_per_l_V0  \\\n",
      "0         1       control                             35.6                             20.5                             13.4                            405   \n",
      "1         2  intervention                               15                             13.3                                x                            321   \n",
      "2         3  intervention                             22.8                             26.5                             14.5                            294   \n",
      "3         4       control                              8.9                              7.5                                x                            380   \n",
      "4         5  intervention                             17.6                             15.7                              9.5                            294   \n",
      "\n",
      "  Vitamin_B12_nanogramm_per_l_V1 Vitamin_B12_nanogramm_per_l_V2 Zink_HP_microgramm_per_l_V0 Zink_HP_microgramm_per_l_V1 Zink_HP_microgramm_per_l_V2  \\\n",
      "0                            363                            314                           x                         847                         611   \n",
      "1                            295                            339                         623                         625                         737   \n",
      "2                            437                            441                         781                         716                         652   \n",
      "3                            395                              x                         802                         837                           x   \n",
      "4                            275                            281                         561                         691                         614   \n",
      "\n",
      "  beta_carotin_microgramm_per_l_V0 beta_carotin_microgramm_per_l_V1 beta_carotin_microgramm_per_l_V2  bmi_V0  bmi_V1  bmi_V2  bmi_percentile_V0  \\\n",
      "0                              313                              228                              315    19.6    20.8    19.4               38.0   \n",
      "1                              341                              238                              420    20.4    20.8    20.8               50.0   \n",
      "2                              524                              497                                x    21.6    21.5    22.5               53.0   \n",
      "3                              140                              401                                x    21.1    21.1     NaN               77.0   \n",
      "4                              185                              189                              140    33.0    33.0    33.1               99.0   \n",
      "\n",
      "   bmi_percentile_V1  bmi_percentile_V2  globarztvas_V0  globarztvas_V1  globarztvas_V2  globpatvas_V0  globpatvas_V1  globpatvas_V2  kg_V0  kg_V1  kg_V2  \\\n",
      "0               54.0               32.0             0.0             0.0             0.0            2.0            0.0            2.0   56.7   60.0   58.0   \n",
      "1               55.0               53.0             1.5             1.5             1.5            0.0            1.0            1.0   59.1   60.0   60.0   \n",
      "2               53.0               68.0             0.0             0.5             0.0            1.0            1.0            1.0   62.5   62.0   65.0   \n",
      "3               75.0                NaN             0.5             3.0             NaN            1.0            2.0            NaN   61.0   61.0    NaN   \n",
      "4               99.0               99.0             2.0             1.0             1.5            2.0            2.0            2.0   79.8   79.8   80.0   \n",
      "\n",
      "   kg_percentile_V0  kg_percentile_V1  kg_percentile_V2  kl_V0  kl_V1  kl_V2  kl_percentile_V0  kl_percentile_V1  kl_percentile_V2 tv_adhrz_TV1  tv_adhrz_TV2  \n",
      "0              50.0              63.0              48.0  170.0  170.0  173.0              75.0              73.0              88.0    Kontrolle           NaN  \n",
      "1              61.0              64.0              62.0  170.0  170.0  170.0              75.0              74.0              73.0          NaN  Intervention  \n",
      "2              65.0              64.0              73.0  170.0  170.0  170.0              85.0              85.0              85.0          NaN  Intervention  \n",
      "3              89.0              87.0               NaN  170.0  170.0    NaN              94.0              92.0               NaN    Kontrolle           NaN  \n",
      "4              98.0              98.0              98.0  155.5  155.5  155.5              10.0               9.0               8.0            1  Intervention  \n"
     ]
    }
   ],
   "source": [
    "# After pivot_table, columns are a MultiIndex with levels\n",
    "# first level is variable name, second level is time point\n",
    "# We flatten this into single strings to make the dataset easy to export and use\n",
    "df_wide.columns = [f\"{var}_{time}\" for var, time in df_wide.columns]\n",
    "\n",
    "# Bring study_id and group back as regular columns rather than index\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "print(\"Wide data shape\")\n",
    "print(df_wide.shape)\n",
    "\n",
    "print(\"Wide data preview\")\n",
    "print(df_wide.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved wide files\n",
      "/Users/stevenschepanski/Documents/04_ANALYSIS/NutriJIA/data/NutriJIA_SPSS_Showoff_wide.xlsx\n",
      "/Users/stevenschepanski/Documents/04_ANALYSIS/NutriJIA/data/NutriJIA_SPSS_Showoff_wide.csv\n"
     ]
    }
   ],
   "source": [
    "# Export into the data folder of the project by default\n",
    "# Using xlsx preserves the wide structure nicely for SPSS demonstration workflows\n",
    "out_dir = PROJECT_DIR / \"data\" if PROJECT_DIR.exists() else WORK_DIR\n",
    "out_file_xlsx = out_dir / \"NutriJIA_SPSS_Showoff_wide.xlsx\"\n",
    "out_file_csv = out_dir / \"NutriJIA_SPSS_Showoff_wide.csv\"\n",
    "\n",
    "# Write files\n",
    "df_wide.to_excel(out_file_xlsx, index=False)\n",
    "df_wide.to_csv(out_file_csv, index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved wide files\")\n",
    "print(out_file_xlsx)\n",
    "print(out_file_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1   Inspect unique values per column before cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to summarise unique raw values in each column\n",
    "# The goal is to understand what non numeric placeholders exist\n",
    "# We do this before any conversion so we can document what we changed later\n",
    "# We also keep the output compact by showing only columns that have suspicious values\n",
    "\n",
    "def summarise_unique_values(series, max_values=25):\n",
    "    \"\"\"\n",
    "    Create a compact summary of unique raw values in a pandas Series\n",
    "\n",
    "    This is used to detect placeholders like \"x\", \"X\", \".\", \"na\", and so on\n",
    "    We return counts for each distinct value as strings for readability\n",
    "    \"\"\"\n",
    "    value_counts = (\n",
    "        series.astype(str)\n",
    "        .str.strip()\n",
    "        .replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "        .value_counts(dropna=False)\n",
    "    )\n",
    "\n",
    "    # Limit output to avoid overwhelming the notebook\n",
    "    out = value_counts.head(max_values)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns that are not identifiers\n",
    "# These are the columns where we expect primarily numeric values\n",
    "id_like_cols = {\"study_id\", \"group\"}\n",
    "data_cols = [c for c in df_wide.columns if c not in id_like_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of object dtype columns in wide dataset\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "# Detect columns that likely contain non numeric entries\n",
    "# We check object dtype columns first as these are the usual suspects\n",
    "object_cols = [c for c in data_cols if df_wide[c].dtype == \"object\"]\n",
    "\n",
    "print(\"Number of object dtype columns in wide dataset\")\n",
    "print(len(object_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of unique value summaries for object columns\n",
    "# This helps you scan quickly for placeholders and unexpected text\n",
    "unique_value_summaries = {}\n",
    "for col in object_cols:\n",
    "    unique_value_summaries[col] = summarise_unique_values(df_wide[col], max_values=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of raw unique values for object dtype columns\n",
      "\n",
      "Column name\n",
      "Transferrin_Sättigung_percent_V0\n",
      "Transferrin_Sättigung_percent_V0\n",
      "35.6    1\n",
      "30.1    1\n",
      "25.2    1\n",
      "23.7    1\n",
      "24.4    1\n",
      "32.5    1\n",
      "57.4    1\n",
      "15.5    1\n",
      "18.7    1\n",
      "40.6    1\n",
      "10.6    1\n",
      "47.4    1\n",
      "15      1\n",
      "48.5    1\n",
      "16.3    1\n",
      "26.4    1\n",
      "43.7    1\n",
      "12.7    1\n",
      "18.9    1\n",
      "17.6    1\n",
      "8.9     1\n",
      "22.8    1\n",
      "29.2    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Transferrin_Sättigung_percent_V1\n",
      "Transferrin_Sättigung_percent_V1\n",
      "20.5    1\n",
      "31.7    1\n",
      "35      1\n",
      "28      1\n",
      "6.5     1\n",
      "32.8    1\n",
      "23.3    1\n",
      "17.7    1\n",
      "14.6    1\n",
      "32.5    1\n",
      "16.4    1\n",
      "20.4    1\n",
      "13.3    1\n",
      "70.8    1\n",
      "27.2    1\n",
      "28.2    1\n",
      "16.5    1\n",
      "22.3    1\n",
      "31.2    1\n",
      "15.7    1\n",
      "7.5     1\n",
      "26.5    1\n",
      "19.3    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Transferrin_Sättigung_percent_V2\n",
      "Transferrin_Sättigung_percent_V2\n",
      "x       5\n",
      "13.4    1\n",
      "33.5    1\n",
      "9.4     1\n",
      "22.5    1\n",
      "7.4     1\n",
      "31.7    1\n",
      "30.8    1\n",
      "24.1    1\n",
      "31.1    1\n",
      "19.5    1\n",
      "47.2    1\n",
      "14.9    1\n",
      "19.1    1\n",
      "26.1    1\n",
      "8.7     1\n",
      "9.5     1\n",
      "14.5    1\n",
      "26.8    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Vitamin_B12_nanogramm_per_l_V0\n",
      "Vitamin_B12_nanogramm_per_l_V0\n",
      "294    2\n",
      "405    1\n",
      "599    1\n",
      "435    1\n",
      "631    1\n",
      "695    1\n",
      "600    1\n",
      "372    1\n",
      "442    1\n",
      "360    1\n",
      "297    1\n",
      "668    1\n",
      "321    1\n",
      "505    1\n",
      "625    1\n",
      "333    1\n",
      "453    1\n",
      "337    1\n",
      "741    1\n",
      "437    1\n",
      "380    1\n",
      "528    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Vitamin_B12_nanogramm_per_l_V1\n",
      "Vitamin_B12_nanogramm_per_l_V1\n",
      "363    1\n",
      "737    1\n",
      "482    1\n",
      "648    1\n",
      "802    1\n",
      "x      1\n",
      "319    1\n",
      "430    1\n",
      "326    1\n",
      "306    1\n",
      "475    1\n",
      "584    1\n",
      "295    1\n",
      "619    1\n",
      "297    1\n",
      "435    1\n",
      "406    1\n",
      "727    1\n",
      "380    1\n",
      "275    1\n",
      "395    1\n",
      "437    1\n",
      "514    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Vitamin_B12_nanogramm_per_l_V2\n",
      "Vitamin_B12_nanogramm_per_l_V2\n",
      "x      4\n",
      "314    1\n",
      "329    1\n",
      "568    1\n",
      "564    1\n",
      "553    1\n",
      "684    1\n",
      "365    1\n",
      "310    1\n",
      "463    1\n",
      "550    1\n",
      "339    1\n",
      "744    1\n",
      "293    1\n",
      "325    1\n",
      "280    1\n",
      "630    1\n",
      "281    1\n",
      "441    1\n",
      "676    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Zink_HP_microgramm_per_l_V0\n",
      "Zink_HP_microgramm_per_l_V0\n",
      "x      1\n",
      "720    1\n",
      "631    1\n",
      "638    1\n",
      "672    1\n",
      "838    1\n",
      "705    1\n",
      "591    1\n",
      "918    1\n",
      "680    1\n",
      "502    1\n",
      "589    1\n",
      "623    1\n",
      "763    1\n",
      "668    1\n",
      "933    1\n",
      "648    1\n",
      "703    1\n",
      "854    1\n",
      "561    1\n",
      "802    1\n",
      "781    1\n",
      "552    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Zink_HP_microgramm_per_l_V1\n",
      "Zink_HP_microgramm_per_l_V1\n",
      "717    2\n",
      "625    2\n",
      "847    1\n",
      "671    1\n",
      "819    1\n",
      "617    1\n",
      "730    1\n",
      "700    1\n",
      "850    1\n",
      "597    1\n",
      "613    1\n",
      "738    1\n",
      "742    1\n",
      "748    1\n",
      "629    1\n",
      "710    1\n",
      "945    1\n",
      "691    1\n",
      "837    1\n",
      "716    1\n",
      "760    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "Zink_HP_microgramm_per_l_V2\n",
      "Zink_HP_microgramm_per_l_V2\n",
      "x      6\n",
      "591    2\n",
      "663    1\n",
      "763    1\n",
      "766    1\n",
      "759    1\n",
      "757    1\n",
      "634    1\n",
      "81     1\n",
      "611    1\n",
      "737    1\n",
      "698    1\n",
      "548    1\n",
      "566    1\n",
      "614    1\n",
      "652    1\n",
      "794    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "beta_carotin_microgramm_per_l_V0\n",
      "beta_carotin_microgramm_per_l_V0\n",
      "313     1\n",
      "719     1\n",
      "280     1\n",
      "182     1\n",
      "351     1\n",
      "1137    1\n",
      "394     1\n",
      "333     1\n",
      "201     1\n",
      "434     1\n",
      "1147    1\n",
      "461     1\n",
      "341     1\n",
      "97      1\n",
      "602     1\n",
      "467     1\n",
      "499     1\n",
      "307     1\n",
      "520     1\n",
      "185     1\n",
      "140     1\n",
      "524     1\n",
      "248     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "beta_carotin_microgramm_per_l_V1\n",
      "beta_carotin_microgramm_per_l_V1\n",
      "261     2\n",
      "228     1\n",
      "238     1\n",
      "425     1\n",
      "341     1\n",
      "1553    1\n",
      "324     1\n",
      "369     1\n",
      "378     1\n",
      "1277    1\n",
      "462     1\n",
      "387     1\n",
      "166     1\n",
      "410     1\n",
      "x       1\n",
      "569     1\n",
      "254     1\n",
      "429     1\n",
      "189     1\n",
      "401     1\n",
      "497     1\n",
      "357     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "beta_carotin_microgramm_per_l_V2\n",
      "beta_carotin_microgramm_per_l_V2\n",
      "x      7\n",
      "315    1\n",
      "330    1\n",
      "323    1\n",
      "209    1\n",
      "337    1\n",
      "425    1\n",
      "700    1\n",
      "203    1\n",
      "68     1\n",
      "420    1\n",
      "300    1\n",
      "451    1\n",
      "513    1\n",
      "253    1\n",
      "140    1\n",
      "206    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "tv_adhrz_TV1\n",
      "tv_adhrz_TV1\n",
      "Kontrolle    11\n",
      "1             3\n",
      "5             3\n",
      "NaN           2\n",
      "3             2\n",
      "2             1\n",
      "2.5           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column name\n",
      "tv_adhrz_TV2\n",
      "tv_adhrz_TV2\n",
      "Intervention    12\n",
      "NaN              2\n",
      "2                2\n",
      "3                2\n",
      "5                2\n",
      "0                1\n",
      "1                1\n",
      "4                1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print a compact overview of object columns and their top unique values\n",
    "# This gives you quick insight into which columns need cleaning rules\n",
    "print(\"Preview of raw unique values for object dtype columns\")\n",
    "for col, vc in unique_value_summaries.items():\n",
    "    print(\"\\nColumn name\")\n",
    "    print(col)\n",
    "    print(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2   Clean data types and harmonise missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now translate what we saw in the unique value inspection into explicit rules\n",
    "# Rule set for numeric clinical and laboratory outcomes\n",
    "# Values are numeric strings\n",
    "# Placeholder \"x\" indicates missing information and must become a real missing value\n",
    "#\n",
    "# Rule set for adherence variables tv_adhrz_TV1 and tv_adhrz_TV2\n",
    "# These variables contain true adherence values as numbers\n",
    "# They also contain group labels as placeholders for not applicable by design\n",
    "# Therefore \"Kontrolle\" and \"Intervention\" are not adherence values and must become missing\n",
    "# After recoding those labels to missing, the remaining values can be coerced to numeric\n",
    "\n",
    "# Tokens interpreted as missing values across the dataset\n",
    "MISSING_TOKENS = {\n",
    "    \"x\",\n",
    "    \"X\",\n",
    "    \"\",\n",
    "    \" \",\n",
    "    \"na\",\n",
    "    \"n/a\",\n",
    "    \"NA\",\n",
    "    \"N/A\",\n",
    "    \"nan\",\n",
    "    \"NaN\",\n",
    "    \".\",\n",
    "    \"-\",\n",
    "    \"None\",\n",
    "    \"NULL\",\n",
    "    \"null\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens that are specific to the adherence variables and indicate not applicable by design\n",
    "# These must be treated as missing for adherence because they do not encode a score\n",
    "ADHERENCE_NOT_APPLICABLE_TOKENS = {\n",
    "    \"Kontrolle\",\n",
    "    \"Intervention\",\n",
    "    \"kontrolle\",\n",
    "    \"intervention\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to clean string columns\n",
    "def clean_string_tokens(series):\n",
    "    \"\"\"\n",
    "    Clean a Series as text and standardise missing values.\n",
    "\n",
    "    This helper is used as the first step for all columns\n",
    "    It does not coerce to numeric\n",
    "    It only strips whitespace, harmonises decimal commas, and replaces missing tokens\n",
    "    \"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    s = s.str.replace(\",\", \".\", regex=False)\n",
    "    s = s.replace(list(MISSING_TOKENS), np.nan)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and coerce to numeric\n",
    "def clean_to_numeric(series):\n",
    "    \"\"\"\n",
    "    Clean a Series and coerce it to numeric.\n",
    "\n",
    "    This should be used for columns that are supposed to be quantitative\n",
    "    Any non parseable entries become missing values\n",
    "    \"\"\"\n",
    "    s = clean_string_tokens(series)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean adherence columns and coerce to numeric\n",
    "def clean_adherence_to_numeric(series):\n",
    "    \"\"\"\n",
    "    Clean adherence columns and coerce to numeric.\n",
    "\n",
    "    Special handling\n",
    "    Group label placeholders are treated as not applicable\n",
    "    Those placeholders become missing\n",
    "    Remaining values are coerced to numeric\n",
    "    \"\"\"\n",
    "    s = clean_string_tokens(series)\n",
    "\n",
    "    # Replace not applicable placeholders with missing\n",
    "    s = s.replace(list(ADHERENCE_NOT_APPLICABLE_TOKENS), np.nan)\n",
    "\n",
    "    # Coerce to numeric\n",
    "    return pd.to_numeric(s, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cleaned copy of the wide dataset\n",
    "# We keep df_wide as the untouched raw wide dataset for auditing and reproducibility\n",
    "df_wide_clean = df_wide.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify adherence columns by prefix\n",
    "tv_adhrz_cols = [c for c in df_wide_clean.columns if c.lower().startswith(\"tv_adhrz\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify identifier columns that should not be converted\n",
    "id_cols_clean = [\"study_id\", \"group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate numeric columns\n",
      "['Transferrin_Sättigung_percent_V0', 'Transferrin_Sättigung_percent_V1', 'Transferrin_Sättigung_percent_V2', 'Vitamin_B12_nanogramm_per_l_V0', 'Vitamin_B12_nanogramm_per_l_V1', 'Vitamin_B12_nanogramm_per_l_V2', 'Zink_HP_microgramm_per_l_V0', 'Zink_HP_microgramm_per_l_V1', 'Zink_HP_microgramm_per_l_V2', 'beta_carotin_microgramm_per_l_V0', 'beta_carotin_microgramm_per_l_V1', 'beta_carotin_microgramm_per_l_V2', 'bmi_V0', 'bmi_V1', 'bmi_V2', 'bmi_percentile_V0', 'bmi_percentile_V1', 'bmi_percentile_V2', 'globarztvas_V0', 'globarztvas_V1', 'globarztvas_V2', 'globpatvas_V0', 'globpatvas_V1', 'globpatvas_V2', 'kg_V0', 'kg_V1', 'kg_V2', 'kg_percentile_V0', 'kg_percentile_V1', 'kg_percentile_V2', 'kl_V0', 'kl_V1', 'kl_V2', 'kl_percentile_V0', 'kl_percentile_V1', 'kl_percentile_V2']\n",
      "Adherence columns\n",
      "['tv_adhrz_TV1', 'tv_adhrz_TV2']\n"
     ]
    }
   ],
   "source": [
    "# Identify all remaining columns as candidates for numeric conversion\n",
    "# We will treat adherence columns with their specialised cleaner\n",
    "candidate_numeric_cols = [\n",
    "    c for c in df_wide_clean.columns\n",
    "    if c not in id_cols_clean and c not in tv_adhrz_cols\n",
    "]\n",
    "\n",
    "print(\"Candidate numeric columns\")\n",
    "print(candidate_numeric_cols)\n",
    "\n",
    "print(\"Adherence columns\")\n",
    "print(tv_adhrz_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply numeric cleaning to standard quantitative columns\n",
    "conversion_log = []\n",
    "for col in candidate_numeric_cols:\n",
    "    before_non_missing = df_wide_clean[col].notna().sum()\n",
    "    df_wide_clean[col] = clean_to_numeric(df_wide_clean[col])\n",
    "    after_non_missing = df_wide_clean[col].notna().sum()\n",
    "\n",
    "    conversion_log.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"non_missing_before\": int(before_non_missing),\n",
    "            \"non_missing_after\": int(after_non_missing),\n",
    "            \"new_missing_created\": int(before_non_missing - after_non_missing),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of numeric conversion and missingness changes\n",
      "                              column  non_missing_before  non_missing_after  new_missing_created\n",
      "37                      tv_adhrz_TV2                  21                  9                   12\n",
      "36                      tv_adhrz_TV1                  21                 10                   11\n",
      "11  beta_carotin_microgramm_per_l_V2                  23                 16                    7\n",
      "8        Zink_HP_microgramm_per_l_V2                  23                 17                    6\n",
      "2   Transferrin_Sättigung_percent_V2                  23                 18                    5\n",
      "5     Vitamin_B12_nanogramm_per_l_V2                  23                 19                    4\n",
      "4     Vitamin_B12_nanogramm_per_l_V1                  23                 22                    1\n",
      "6        Zink_HP_microgramm_per_l_V0                  23                 22                    1\n",
      "10  beta_carotin_microgramm_per_l_V1                  23                 22                    1\n",
      "0   Transferrin_Sättigung_percent_V0                  23                 23                    0\n",
      "1   Transferrin_Sättigung_percent_V1                  23                 23                    0\n",
      "3     Vitamin_B12_nanogramm_per_l_V0                  23                 23                    0\n",
      "7        Zink_HP_microgramm_per_l_V1                  23                 23                    0\n",
      "9   beta_carotin_microgramm_per_l_V0                  23                 23                    0\n",
      "12                            bmi_V0                  23                 23                    0\n",
      "13                            bmi_V1                  22                 22                    0\n",
      "14                            bmi_V2                  21                 21                    0\n",
      "15                 bmi_percentile_V0                  23                 23                    0\n",
      "16                 bmi_percentile_V1                  22                 22                    0\n",
      "17                 bmi_percentile_V2                  21                 21                    0\n",
      "18                    globarztvas_V0                  23                 23                    0\n",
      "19                    globarztvas_V1                  23                 23                    0\n",
      "20                    globarztvas_V2                  21                 21                    0\n",
      "21                     globpatvas_V0                  23                 23                    0\n",
      "22                     globpatvas_V1                  23                 23                    0\n",
      "23                     globpatvas_V2                  21                 21                    0\n",
      "24                             kg_V0                  23                 23                    0\n",
      "25                             kg_V1                  22                 22                    0\n",
      "26                             kg_V2                  21                 21                    0\n",
      "27                  kg_percentile_V0                  23                 23                    0\n",
      "28                  kg_percentile_V1                  22                 22                    0\n",
      "29                  kg_percentile_V2                  21                 21                    0\n",
      "30                             kl_V0                  23                 23                    0\n",
      "31                             kl_V1                  22                 22                    0\n",
      "32                             kl_V2                  21                 21                    0\n",
      "33                  kl_percentile_V0                  23                 23                    0\n",
      "34                  kl_percentile_V1                  22                 22                    0\n",
      "35                  kl_percentile_V2                  21                 21                    0\n",
      "Cleaned wide data shape\n",
      "(23, 40)\n",
      "Cleaned wide data preview\n",
      "   study_id         group  Transferrin_Sättigung_percent_V0  Transferrin_Sättigung_percent_V1  Transferrin_Sättigung_percent_V2  \\\n",
      "0         1       control                              35.6                              20.5                              13.4   \n",
      "1         2  intervention                              15.0                              13.3                               NaN   \n",
      "2         3  intervention                              22.8                              26.5                              14.5   \n",
      "3         4       control                               8.9                               7.5                               NaN   \n",
      "4         5  intervention                              17.6                              15.7                               9.5   \n",
      "\n",
      "   Vitamin_B12_nanogramm_per_l_V0  Vitamin_B12_nanogramm_per_l_V1  Vitamin_B12_nanogramm_per_l_V2  Zink_HP_microgramm_per_l_V0  Zink_HP_microgramm_per_l_V1  \\\n",
      "0                             405                           363.0                           314.0                          NaN                          847   \n",
      "1                             321                           295.0                           339.0                        623.0                          625   \n",
      "2                             294                           437.0                           441.0                        781.0                          716   \n",
      "3                             380                           395.0                             NaN                        802.0                          837   \n",
      "4                             294                           275.0                           281.0                        561.0                          691   \n",
      "\n",
      "   Zink_HP_microgramm_per_l_V2  beta_carotin_microgramm_per_l_V0  beta_carotin_microgramm_per_l_V1  beta_carotin_microgramm_per_l_V2  bmi_V0  bmi_V1  bmi_V2  \\\n",
      "0                        611.0                               313                             228.0                             315.0    19.6    20.8    19.4   \n",
      "1                        737.0                               341                             238.0                             420.0    20.4    20.8    20.8   \n",
      "2                        652.0                               524                             497.0                               NaN    21.6    21.5    22.5   \n",
      "3                          NaN                               140                             401.0                               NaN    21.1    21.1     NaN   \n",
      "4                        614.0                               185                             189.0                             140.0    33.0    33.0    33.1   \n",
      "\n",
      "   bmi_percentile_V0  bmi_percentile_V1  bmi_percentile_V2  globarztvas_V0  globarztvas_V1  globarztvas_V2  globpatvas_V0  globpatvas_V1  globpatvas_V2  \\\n",
      "0               38.0               54.0               32.0             0.0             0.0             0.0            2.0            0.0            2.0   \n",
      "1               50.0               55.0               53.0             1.5             1.5             1.5            0.0            1.0            1.0   \n",
      "2               53.0               53.0               68.0             0.0             0.5             0.0            1.0            1.0            1.0   \n",
      "3               77.0               75.0                NaN             0.5             3.0             NaN            1.0            2.0            NaN   \n",
      "4               99.0               99.0               99.0             2.0             1.0             1.5            2.0            2.0            2.0   \n",
      "\n",
      "   kg_V0  kg_V1  kg_V2  kg_percentile_V0  kg_percentile_V1  kg_percentile_V2  kl_V0  kl_V1  kl_V2  kl_percentile_V0  kl_percentile_V1  kl_percentile_V2  \\\n",
      "0   56.7   60.0   58.0              50.0              63.0              48.0  170.0  170.0  173.0              75.0              73.0              88.0   \n",
      "1   59.1   60.0   60.0              61.0              64.0              62.0  170.0  170.0  170.0              75.0              74.0              73.0   \n",
      "2   62.5   62.0   65.0              65.0              64.0              73.0  170.0  170.0  170.0              85.0              85.0              85.0   \n",
      "3   61.0   61.0    NaN              89.0              87.0               NaN  170.0  170.0    NaN              94.0              92.0               NaN   \n",
      "4   79.8   79.8   80.0              98.0              98.0              98.0  155.5  155.5  155.5              10.0               9.0               8.0   \n",
      "\n",
      "   tv_adhrz_TV1  tv_adhrz_TV2  \n",
      "0           NaN           NaN  \n",
      "1           NaN           NaN  \n",
      "2           NaN           NaN  \n",
      "3           NaN           NaN  \n",
      "4           1.0           NaN  \n",
      "Cleaned wide data types preview\n",
      "study_id                              int64\n",
      "group                                object\n",
      "Transferrin_Sättigung_percent_V0    float64\n",
      "Transferrin_Sättigung_percent_V1    float64\n",
      "Transferrin_Sättigung_percent_V2    float64\n",
      "Vitamin_B12_nanogramm_per_l_V0        int64\n",
      "Vitamin_B12_nanogramm_per_l_V1      float64\n",
      "Vitamin_B12_nanogramm_per_l_V2      float64\n",
      "Zink_HP_microgramm_per_l_V0         float64\n",
      "Zink_HP_microgramm_per_l_V1           int64\n",
      "Zink_HP_microgramm_per_l_V2         float64\n",
      "beta_carotin_microgramm_per_l_V0      int64\n",
      "beta_carotin_microgramm_per_l_V1    float64\n",
      "beta_carotin_microgramm_per_l_V2    float64\n",
      "bmi_V0                              float64\n",
      "bmi_V1                              float64\n",
      "bmi_V2                              float64\n",
      "bmi_percentile_V0                   float64\n",
      "bmi_percentile_V1                   float64\n",
      "bmi_percentile_V2                   float64\n",
      "globarztvas_V0                      float64\n",
      "globarztvas_V1                      float64\n",
      "globarztvas_V2                      float64\n",
      "globpatvas_V0                       float64\n",
      "globpatvas_V1                       float64\n",
      "globpatvas_V2                       float64\n",
      "kg_V0                               float64\n",
      "kg_V1                               float64\n",
      "kg_V2                               float64\n",
      "kg_percentile_V0                    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Apply specialised cleaning to adherence columns\n",
    "# The conversion log is extended so we can diagnose if unexpected information is lost\n",
    "for col in tv_adhrz_cols:\n",
    "    before_non_missing = df_wide_clean[col].notna().sum()\n",
    "    df_wide_clean[col] = clean_adherence_to_numeric(df_wide_clean[col])\n",
    "    after_non_missing = df_wide_clean[col].notna().sum()\n",
    "\n",
    "    conversion_log.append(\n",
    "        {\n",
    "            \"column\": col,\n",
    "            \"non_missing_before\": int(before_non_missing),\n",
    "            \"non_missing_after\": int(after_non_missing),\n",
    "            \"new_missing_created\": int(before_non_missing - after_non_missing),\n",
    "        }\n",
    "    )\n",
    "\n",
    "conversion_log_df = (\n",
    "    pd.DataFrame(conversion_log)\n",
    "    .sort_values([\"new_missing_created\", \"column\"], ascending=[False, True])\n",
    ")\n",
    "\n",
    "print(\"Summary of numeric conversion and missingness changes\")\n",
    "print(conversion_log_df)\n",
    "\n",
    "print(\"Cleaned wide data shape\")\n",
    "print(df_wide_clean.shape)\n",
    "\n",
    "print(\"Cleaned wide data preview\")\n",
    "print(df_wide_clean.head())\n",
    "\n",
    "print(\"Cleaned wide data types preview\")\n",
    "print(df_wide_clean.dtypes.head(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2a   Quick validation checks for adherence logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Non missing adherence counts by group for column\n",
      "tv_adhrz_TV1\n",
      "          group  n_non_missing\n",
      "0       control              0\n",
      "1  intervention             10\n",
      "Basic adherence summary by group for column\n",
      "tv_adhrz_TV1\n",
      "          group   n  mean      sd  median  min  max\n",
      "0       control   0   NaN     NaN     NaN  NaN  NaN\n",
      "1  intervention  10  2.85  1.6675    2.75  1.0  5.0\n",
      "\n",
      "Non missing adherence counts by group for column\n",
      "tv_adhrz_TV2\n",
      "          group  n_non_missing\n",
      "0       control              9\n",
      "1  intervention              0\n",
      "Basic adherence summary by group for column\n",
      "tv_adhrz_TV2\n",
      "          group  n      mean        sd  median  min  max\n",
      "0       control  9  2.777778  1.715938     3.0  0.0  5.0\n",
      "1  intervention  0       NaN       NaN     NaN  NaN  NaN\n"
     ]
    }
   ],
   "source": [
    "# These checks are designed to confirm your interpretation of the protocol\n",
    "# We expect adherence at TV1 to be mostly available in the intervention group\n",
    "# We expect adherence at TV2 to be mostly available in the control group\n",
    "#\n",
    "# Because the dataset is small, we summarise counts of non missing by group\n",
    "\n",
    "if len(tv_adhrz_cols) > 0:\n",
    "    for col in tv_adhrz_cols:\n",
    "        print(\"\\nNon missing adherence counts by group for column\")\n",
    "        print(col)\n",
    "        tmp = (\n",
    "            df_wide_clean\n",
    "            .groupby(\"group\")[col]\n",
    "            .apply(lambda x: x.notna().sum())\n",
    "            .reset_index(name=\"n_non_missing\")\n",
    "        )\n",
    "        print(tmp)\n",
    "\n",
    "        print(\"Basic adherence summary by group for column\")\n",
    "        print(col)\n",
    "        tmp2 = (\n",
    "            df_wide_clean\n",
    "            .groupby(\"group\")[col]\n",
    "            .agg(\n",
    "                n=\"count\",\n",
    "                mean=\"mean\",\n",
    "                sd=\"std\",\n",
    "                median=\"median\",\n",
    "                min=\"min\",\n",
    "                max=\"max\",\n",
    "            )\n",
    "            .reset_index()\n",
    "        )\n",
    "        print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3   Create analysis ready datasets for primary outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary outcomes are globpatvas and globarztvas\n",
    "# Baseline is V0 and endpoint is V2\n",
    "# We create explicit baseline and endpoint variables to avoid mistakes later\n",
    "\n",
    "PRIMARY_OUTCOMES = [\"globpatvas\", \"globarztvas\"]\n",
    "BASELINE_TP = \"V0\"\n",
    "ENDPOINT_TP = \"V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to construct wide column names\n",
    "def wide_name(var, tp):\n",
    "    \"\"\"\n",
    "    Construct the expected wide column name given a variable and time point\n",
    "    Example globpatvas_V0\n",
    "    \"\"\"\n",
    "    return f\"{var}_{tp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a list of required columns for primary outcome analyses\n",
    "required_primary_cols = [\"study_id\", \"group\"]\n",
    "for var in PRIMARY_OUTCOMES:\n",
    "    required_primary_cols.append(wide_name(var, BASELINE_TP))\n",
    "    required_primary_cols.append(wide_name(var, ENDPOINT_TP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all required primary outcome columns are present\n",
    "missing_primary_cols = [c for c in required_primary_cols if c not in df_wide_clean.columns]\n",
    "if len(missing_primary_cols) > 0:\n",
    "    print(\"Error primary outcome columns missing from cleaned dataset\")\n",
    "    print(missing_primary_cols)\n",
    "    raise KeyError(\"Primary outcome columns not found in df_wide_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary outcomes analysis dataset preview\n",
      "   study_id         group  globpatvas_baseline  globpatvas_endpoint  globarztvas_baseline  globarztvas_endpoint\n",
      "0         1       control                  2.0                  2.0                   0.0                   0.0\n",
      "1         2  intervention                  0.0                  1.0                   1.5                   1.5\n",
      "2         3  intervention                  1.0                  1.0                   0.0                   0.0\n",
      "3         4       control                  1.0                  NaN                   0.5                   NaN\n",
      "4         5  intervention                  2.0                  2.0                   2.0                   1.5\n"
     ]
    }
   ],
   "source": [
    "# Create a primary outcomes analysis dataset\n",
    "df_primary = df_wide_clean[required_primary_cols].copy()\n",
    "\n",
    "df_primary = df_primary.rename(\n",
    "    columns={\n",
    "        wide_name(\"globpatvas\", BASELINE_TP): \"globpatvas_baseline\",\n",
    "        wide_name(\"globpatvas\", ENDPOINT_TP): \"globpatvas_endpoint\",\n",
    "        wide_name(\"globarztvas\", BASELINE_TP): \"globarztvas_baseline\",\n",
    "        wide_name(\"globarztvas\", ENDPOINT_TP): \"globarztvas_endpoint\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Primary outcomes analysis dataset preview\")\n",
    "print(df_primary.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness counts for primary outcomes\n",
      "complete_globpatvas     21\n",
      "complete_globarztvas    21\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Simple completeness flags\n",
    "# These are useful for reporting and for deciding which analysis set is used later\n",
    "df_primary[\"complete_globpatvas\"] = df_primary[[\"globpatvas_baseline\", \"globpatvas_endpoint\"]].notna().all(axis=1)\n",
    "df_primary[\"complete_globarztvas\"] = df_primary[[\"globarztvas_baseline\", \"globarztvas_endpoint\"]].notna().all(axis=1)\n",
    "\n",
    "print(\"Completeness counts for primary outcomes\")\n",
    "print(df_primary[[\"complete_globpatvas\", \"complete_globarztvas\"]].sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4   Descriptive baseline tables stratified by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline summaries are descriptive only\n",
    "# They are not inferential tests\n",
    "# With very small n, these summaries should be interpreted cautiously\n",
    "\n",
    "def describe_by_group(df, value_col):\n",
    "    \"\"\"\n",
    "    Produce descriptive statistics for a single variable stratified by group\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df\n",
    "        .groupby(\"group\")[value_col]\n",
    "        .agg(\n",
    "            n=\"count\",\n",
    "            mean=\"mean\",\n",
    "            sd=\"std\",\n",
    "            median=\"median\",\n",
    "            min=\"min\",\n",
    "            max=\"max\",\n",
    "        )\n",
    "        .reset_index()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline descriptive table for globpatvas\n",
      "          group   n      mean        sd  median  min  max\n",
      "0       control  11  2.772727  1.941181     2.0  1.0  7.0\n",
      "1  intervention  12  1.375000  1.720531     1.0  0.0  6.0\n",
      "Baseline descriptive table for globarztvas\n",
      "          group   n      mean        sd  median  min  max\n",
      "0       control  11  0.636364  1.026911     0.5  0.0  3.5\n",
      "1  intervention  12  0.583333  0.925235     0.0  0.0  2.5\n"
     ]
    }
   ],
   "source": [
    "# Baseline descriptive tables\n",
    "print(\"Baseline descriptive table for globpatvas\")\n",
    "print(describe_by_group(df_primary, \"globpatvas_baseline\"))\n",
    "\n",
    "print(\"Baseline descriptive table for globarztvas\")\n",
    "print(describe_by_group(df_primary, \"globarztvas_baseline\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
